{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = open(\"FTL_EMNIST_1.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "from numpy import dstack \n",
    "import pandas as pd\n",
    "from keras.layers import (Conv2D, Dense, Dropout, Flatten, GaussianNoise,\n",
    "                          MaxPooling2D, MaxPool2D , Activation)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot\n",
    "from numpy import dstack, mean, std\n",
    "from pandas import read_csv\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from scipy.spatial.distance import euclidean as euc\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240000, 28, 28)\n",
      "(40000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from emnist import extract_training_samples, extract_test_samples\n",
    "X_train, y_train = extract_training_samples('digits')\n",
    "X_test, y_test = extract_test_samples('digits')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# ---------- SETTINGS ----------#\n",
    "# ----------------------------- #\n",
    "\n",
    "NUM_Clients = 10 # number of clients contributing per training round\n",
    "\n",
    "# ML\n",
    "Cluster_Size = 1000 # max client dataset size for training\n",
    "Batch_Size = 32\n",
    "NUM_Epoch = 1\n",
    "verbose = 1\n",
    "TL_Epochs = 1\n",
    "\n",
    "# Krum\n",
    "krum_f = 0.00 # percentage of byzantine nodes\n",
    "\n",
    "# Differential Privacy\n",
    "Gaussian_Noise = False\n",
    "Gaussian_Noise_Std_Dev = 0.20\n",
    "\n",
    "Gradient_Clipping = False\n",
    "Clip_Norm = 0.60\n",
    "\n",
    "Gradient_Pruning = False\n",
    "initial_sparsity = 0.00\n",
    "final_sparsity = 0.50\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- #\n",
    "# ----------------------------- #\n",
    "# ----------------------------- #\n",
    "def settings():\n",
    "    global NUM_Clients,Cluster_Size,Batch_Size,NUM_Epoch,verbose,krum_f,Gaussian_Noise,Gaussian_Noise_Std_Dev,Gradient_Clipping,Clip_Norm,Gradient_Pruning,initial_sparsity,final_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size :  (240000, 28, 28, 1)\n",
      "Data size :  (40000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(X, y):\n",
    "\n",
    "    X = X.reshape(-1,28 ,28,1)\n",
    "    X = X/255.0\n",
    "    y = to_categorical(y, num_classes=10)\n",
    "    \n",
    "    print('Data size : ', X.shape)\n",
    "    return X, y\n",
    "\n",
    "DF_Train_X,DF_Train_Y = preprocess(X_train, y_train)\n",
    "DF_Test_X,DF_Test_Y = preprocess(X_test, y_test) \n",
    "\n",
    "def split_70_30(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "    return  X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name, X_train, y_train, globalId):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[0], X_train.shape[1], y_train.shape[0]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(n_timesteps / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "              'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                       final_sparsity=final_sparsity,\n",
    "                                                                       begin_step=0,\n",
    "                                                                       end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "          tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "          tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.built = True\n",
    "\n",
    "    if globalId != 1:\n",
    "        model.load_weights(\"./weights/global\"+str(globalId)+\".h5\")\n",
    "\n",
    "    if Gradient_Clipping == True:\n",
    "        opt = keras.optimizers.Adam(clipnorm=Clip_Norm)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1, callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1)\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\"+str(name)+\".h5\")\n",
    "    return n_timesteps, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(m, n):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    # Finds eucledian distance between two ML models m & n\n",
    "    distance = []\n",
    "    for i in range(len(m)):\n",
    "        distance.append(euc(m[i].reshape(-1,1), n[i].reshape(-1,1)))\n",
    "    distance = sum(distance)/len(m)\n",
    "    return distance\n",
    "\n",
    "def saveModel(weight, n):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    \n",
    "    num_classes=len(np.unique(DF_Test_Y))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "    if Gaussian_Noise == True: model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(Cluster_Size/Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "          # Define model for pruning.\n",
    "        pruning_params = {\n",
    "            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                        final_sparsity=final_sparsity,\n",
    "                                                                        begin_step=0,\n",
    "                                                                        end_step=end_step)\n",
    "          }\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "            tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.set_weights(weight)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    scores = model.evaluate(DF_Test_X, DF_Test_Y)\n",
    "\n",
    "    print(\"Saved Model Loss: \", scores[0])        #Loss\n",
    "    print(\"Saved Model Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    fpath = \"./weights/global\"+str(n)+\".h5\"\n",
    "    model.save(fpath)\n",
    "    return scores[0], scores[1]\n",
    "\n",
    "def getDataLen(trainingDict):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    n = 0\n",
    "    for w in trainingDict:\n",
    "        n += trainingDict[w]\n",
    "#     print('Total number of data points after this round: ', n)\n",
    "    return n\n",
    "\n",
    "def assignWeights(trainingDf, trainingDict):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    n = getDataLen(trainingDict)\n",
    "    trainingDf['Weightage'] = trainingDf['DataSize'].apply(lambda x: x/n)\n",
    "    return trainingDf, n\n",
    "    \n",
    "def scale(weight, scaler):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    scaledWeights = []\n",
    "    for i in range(len(weight)):\n",
    "        scaledWeights.append(scaler * weight[i])\n",
    "    return scaledWeights\n",
    "\n",
    "def getScaledWeight(d, scaler):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        model = prune_low_magnitude(model)\n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return scale(weight, scaler)\n",
    "\n",
    "def getWeight(d):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        model = prune_low_magnitude(model)\n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return weight\n",
    "\n",
    "def avgWeights(scaledWeights):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    avg = list()\n",
    "    for weight_list_tuple in zip(*scaledWeights):\n",
    "        layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0)\n",
    "        avg.append(layer_mean)\n",
    "    return avg\n",
    "\n",
    "def FedAvg(trainingDict):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize']) \n",
    "    models = list(trainingDict.keys())\n",
    "    scaledWeights = []\n",
    "    trainingDf, dataLen = assignWeights(trainingDf, trainingDict)\n",
    "    for m in models:\n",
    "        scaledWeights.append(getScaledWeight(m, trainingDf.loc[m]['Weightage']))\n",
    "    fedAvgWeight = avgWeights(scaledWeights)\n",
    "    return fedAvgWeight, dataLen\n",
    "\n",
    "def MK(trainingDict, b):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    print('MK Training Dict: ', trainingDict)\n",
    "    models = list(trainingDict.keys())\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize'])\n",
    "    l_weights = []\n",
    "    g_weight = {}\n",
    "    for m in models:\n",
    "        if 'global' in m:\n",
    "            g_weight['name'] = m\n",
    "            g_weight['weight'] = getWeight(m)\n",
    "        else:\n",
    "            l_weights.append({\n",
    "                'name': m,\n",
    "                'weight': getWeight(m)\n",
    "            })\n",
    "    \n",
    "    scores = {}\n",
    "#     if (g_weight == {}):\n",
    "#         return -1,-1\n",
    "    for m in l_weights:\n",
    "        scores[m['name']] = euclidean(m['weight'], g_weight['weight'])\n",
    "    sortedScores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1])}\n",
    "\n",
    "    b = int(len(scores)*b)\n",
    "    \n",
    "    selected = []\n",
    "    for i in range(b):\n",
    "        selected.append((sortedScores.popitem())[0])\n",
    "\n",
    "    newDict = {}\n",
    "    for i in trainingDict.keys():\n",
    "        if (((i not in selected) and ('global' not in i))):\n",
    "            newDict[i] = trainingDict[i]\n",
    "\n",
    "    print('Selections: ', newDict)\n",
    "    NewGlobal, dataLen = FedAvg(newDict)\n",
    "    return NewGlobal, dataLen\n",
    "\n",
    "def TransferLearn(name, X_train, X_test,y_train,y_test):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    # X_train, y_train = preprocess(traindf)\n",
    "    # X_test, y_test = preprocess(testdf)\n",
    "\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[0], X_train.shape[1], y_train.shape[0]\n",
    "\n",
    "    inner_model = Sequential(\n",
    "    [\n",
    "        Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1))\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(inner_model)\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(n_timesteps / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "              'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                       final_sparsity=final_sparsity,\n",
    "                                                                       begin_step=0,\n",
    "                                                                       end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "          tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "          tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.load_weights(\"./weights/global\"+str(curr_global)+\".h5\")\n",
    "\n",
    "    if Gradient_Clipping == True:\n",
    "        opt = keras.optimizers.Adam(clipnorm=Clip_Norm)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    for layer in inner_model.layers:#freezing layers to retain the weights \n",
    "        layer.trainable = False\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        history = model.fit(X_train, y_train, epochs=TL_Epochs, batch_size=Batch_Size, verbose=1, callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, epochs=TL_Epochs, batch_size=Batch_Size, verbose=1)\n",
    "\n",
    "    history2 = model.evaluate(X_test, y_test, batch_size=Batch_Size, verbose=1)\n",
    "\n",
    "    test_accuracy = history2[1]\n",
    "    print(f\"Subject Test Accuracy Post Transfer Learning is {test_accuracy}\")\n",
    "\n",
    "    nofedAcc = get_subject_testacc_before_TL(X_test, y_test, curr_global)\n",
    "\n",
    "    getCent = get_own_individual_acc(X_train, y_train, curr_global)\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\"+str(name)+\".h5\")\n",
    "    return n_timesteps, model\n",
    "\n",
    "def get_subject_testacc_before_TL(X_test, y_test, curr_global):\n",
    "    \n",
    "    global curr_local\n",
    "#     global curr_global\n",
    "        \n",
    "    num_classes=len(np.unique(y_test))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(Cluster_Size / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                    final_sparsity=final_sparsity,\n",
    "                                                                    begin_step=0,\n",
    "                                                                    end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.load_weights(\"./weights/global\"+str(curr_global)+\".h5\")\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "      \n",
    "    scores = model.evaluate(X_test,y_test)\n",
    "    print(\"Subject Testing Loss before TL: \", scores[0])        #Loss\n",
    "    print(\"Subject Testing Accuracy before TL: \", scores[1])    #Accuracy\n",
    "    \n",
    "def get_own_individual_acc(X_train, y_train, curr_global):\n",
    "    \n",
    "    global curr_local\n",
    "#     global curr_global\n",
    "\n",
    "    X_test, y_test = DF_Test_X,DF_Test_Y\n",
    "\n",
    "    num_classes=len(np.unique(y_test))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(16500 / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                    final_sparsity=final_sparsity,\n",
    "                                                                    begin_step=0,\n",
    "                                                                    end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "            tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # model.load_weights(\"./weights/global\"+str(curr_global)+\".h5\")\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1, callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1)\n",
    "      \n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    print(\"Invidual Loss without TL: \", scores[0])        #Loss\n",
    "    print(\"invidual Accuracy without TL: \", scores[1])    #Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    global NUM_Clients,Cluster_Size,Batch_Size,NUM_Epoch,verbose,krum_f,Gaussian_Noise,Gaussian_Noise_Std_Dev,Gradient_Clipping,Clip_Norm,Gradient_Pruning,initial_sparsity,final_sparsity\n",
    "    \n",
    "    num_iter = 2\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        print(f\"-----------------------Repeating entire process the {i}th time-------------------\")\n",
    "        global curr_local\n",
    "        global curr_global\n",
    "        \n",
    "        print('----------------------------------------')\n",
    "        print('Number of Clients: ', NUM_Clients)\n",
    "        print('Cluster Size: ', Cluster_Size)\n",
    "        print('Batch Size: ', Batch_Size)\n",
    "        print('Number of Local Epochs: ', NUM_Epoch)\n",
    "        print('F: ', krum_f)\n",
    "        print('Gaussian_Noise: ', Gaussian_Noise)\n",
    "        if Gaussian_Noise: \n",
    "            print('Noise Std Dev: ', Gaussian_Noise_Std_Dev)\n",
    "        print('Gradient_Clipping: ', Gradient_Clipping)\n",
    "        if Gradient_Clipping: \n",
    "            print('Clip Norm: ', Clip_Norm)\n",
    "        print('Gradient_Pruning: ', Gradient_Pruning)\n",
    "        if Gradient_Pruning: \n",
    "            print('Pruning Sparcity: ', final_sparsity)\n",
    "        print('----------------------------------------')\n",
    "        \n",
    "        curr_local = 0\n",
    "        curr_global = 0\n",
    "        \n",
    "        clients = {}\n",
    "        \n",
    "        Outer_Xtrain,Outer_Xtest,Outer_Ytrain,Outer_Ytest = DF_Train_X,DF_Test_X,DF_Train_Y,DF_Test_Y #happens at global level, only 70% is shared w clients, rest is kept for testing\n",
    "\n",
    "        per_client_data = (len(Outer_Xtrain)//NUM_Clients)\n",
    "        for i in range(NUM_Clients):\n",
    "            \n",
    "            clients[f'X_{i}'] = Outer_Xtrain[per_client_data*i: per_client_data*(i+1)]\n",
    "            clients[f'Y_{i}'] = Outer_Ytrain[per_client_data*i: per_client_data*(i+1)]\n",
    "            clients[f'X_train_{i}'],clients[f'X_test_{i}'],clients[f'Y_train_{i}'],clients[f'Y_test_{i}'] = split_70_30(clients[f'X_{i}'],clients[f'Y_{i}']) \n",
    "\n",
    "        local = {}\n",
    "        loss_array = []\n",
    "        acc_array = []\n",
    "        curr_datalen = 0\n",
    "        \n",
    "        for i in range(0, len(Outer_Xtrain), Cluster_Size):\n",
    "            curr_datalen += Cluster_Size\n",
    "            print(\"Total Data Used: \", curr_datalen)\n",
    "            \n",
    "            if int(curr_global) == 0:\n",
    "                curr_global += 1\n",
    "                print('Current Global: ', curr_global)\n",
    "                name = 'global' + str(curr_global)\n",
    "                X_train_fed, y_train_fed = Outer_Xtrain[i:i+Cluster_Size],Outer_Ytrain[i:i+Cluster_Size]\n",
    "                l, m = train(name, X_train_fed, y_train_fed, curr_global)\n",
    "                local[name] = l\n",
    "                \n",
    "            else:\n",
    "                print('Current Local: ', curr_local)\n",
    "                name = str('local'+str(curr_local))\n",
    "                curr_local += 1\n",
    "                X_train_fed, y_train_fed = Outer_Xtrain[i:i+Cluster_Size],Outer_Ytrain[i:i+Cluster_Size]\n",
    "                if X_train_fed.shape[0]<=Cluster_Size-1:\n",
    "                    break\n",
    "                l, m = train(name, X_train_fed, y_train_fed, curr_global)\n",
    "                local[name] = l\n",
    "                \n",
    "                if (int(curr_local)% NUM_Clients == 0):\n",
    "                    curr_global += 1\n",
    "                    print('Current Global: ', curr_global)\n",
    "                    name = 'global' + str(curr_global)\n",
    "                    m, l = MK(local, krum_f)\n",
    "                    loss, acc = saveModel(m, curr_global)\n",
    "                    loss_array.append(loss)\n",
    "                    acc_array.append(acc)\n",
    "                    local = {}\n",
    "                    local[name] = l\n",
    "\n",
    "        print(\"Global Accuracy Array: \", acc_array)\n",
    "          \n",
    "                \n",
    "        for j in range(NUM_Clients):\n",
    "            print(\"calling tflearn for client \", j)\n",
    "            TransferLearn(f\"C{j}\", clients[f'X_train_{j}'], clients[f'X_test_{j}'],clients[f'Y_train_{j}'],clients[f'Y_test_{j}'])\n",
    "\n",
    "        NUM_Clients+=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Repeating entire process the 0th time-------------------\n",
      "----------------------------------------\n",
      "Number of Clients:  10\n",
      "Cluster Size:  1000\n",
      "Batch Size:  32\n",
      "Number of Local Epochs:  1\n",
      "F:  0.0\n",
      "Gaussian_Noise:  False\n",
      "Gradient_Clipping:  False\n",
      "Gradient_Pruning:  False\n",
      "----------------------------------------\n",
      "Total Data Used:  1000\n",
      "Current Global:  1\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.1546 - accuracy: 0.2500\n",
      "Total Data Used:  2000\n",
      "Current Local:  0\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 2.1243 - accuracy: 0.2652\n",
      "Total Data Used:  3000\n",
      "Current Local:  1\n",
      "28/32 [=========================>....] - ETA: 0s - loss: 2.1805 - accuracy: 0.2274"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ecd396ae1e7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a4391dede452>\u001b[0m in \u001b[0;36minit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mX_train_fed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mCluster_Size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_fed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_global\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mlocal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-47e05c75f009>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(name, X_train, y_train, globalId)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBatch_Size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_Epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBatch_Size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_Epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m#Saving Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
