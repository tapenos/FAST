{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.stdout = open(\"FTL_MNIST_1.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "from numpy import dstack \n",
    "import pandas as pd\n",
    "from keras.layers import (Conv2D, Dense, Dropout, Flatten, GaussianNoise,\n",
    "                          MaxPooling2D, MaxPool2D , Activation)\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot\n",
    "from numpy import dstack, mean, std\n",
    "from pandas import read_csv\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from scipy.spatial.distance import euclidean as euc\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_train = pd.read_csv('mnist_train.csv')\n",
    "DF_test = pd.read_csv('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# ---------- SETTINGS ----------#\n",
    "# ----------------------------- #\n",
    "\n",
    "NUM_Clients = 5 # number of clients contributing per training round\n",
    "\n",
    "# ML\n",
    "Cluster_Size = 100 # max client dataset size for training\n",
    "Batch_Size = 32\n",
    "NUM_Epoch = 3\n",
    "verbose = 1\n",
    "TL_Epochs = 1\n",
    "\n",
    "# Krum\n",
    "krum_f = 0.00 # percentage of byzantine nodes\n",
    "\n",
    "# Differential Privacy\n",
    "Gaussian_Noise = False\n",
    "Gaussian_Noise_Std_Dev = 0.20\n",
    "\n",
    "Gradient_Clipping = False\n",
    "Clip_Norm = 0.60\n",
    "\n",
    "Gradient_Pruning = False\n",
    "initial_sparsity = 0.00\n",
    "final_sparsity = 0.50\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- #\n",
    "# ----------------------------- #\n",
    "# ----------------------------- #\n",
    "def settings():\n",
    "    global NUM_Clients,Cluster_Size,Batch_Size,NUM_Epoch,verbose,krum_f,Gaussian_Noise,Gaussian_Noise_Std_Dev,Gradient_Clipping,Clip_Norm,Gradient_Pruning,initial_sparsity,final_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size :  (60000, 28, 28, 1)\n",
      "Data size :  (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    y = df.label\n",
    "    X = df.drop(['label'], axis=1)\n",
    "    X = X.values.reshape(-1,28 ,28,1)\n",
    "    X = X/255.0\n",
    "    y = to_categorical(y, num_classes=10)\n",
    "    \n",
    "    print('Data size : ', X.shape)\n",
    "    return X, y\n",
    "\n",
    "DF_Train_X,DF_Train_Y = preprocess(DF_train)\n",
    "DF_Test_X,DF_Test_Y = preprocess(DF_test) \n",
    "\n",
    "def split_70_30(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "    return  X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name, X_train, y_train, globalId):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[0], X_train.shape[1], y_train.shape[0]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(n_timesteps / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "              'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                       final_sparsity=final_sparsity,\n",
    "                                                                       begin_step=0,\n",
    "                                                                       end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "          tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "          tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.built = True\n",
    "\n",
    "    if globalId != 1:\n",
    "        model.load_weights(\"./weights/global\"+str(globalId)+\".h5\")\n",
    "\n",
    "    if Gradient_Clipping == True:\n",
    "        opt = keras.optimizers.Adam(clipnorm=Clip_Norm)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1, callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1)\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\"+str(name)+\".h5\")\n",
    "    return n_timesteps, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(m, n):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    # Finds eucledian distance between two ML models m & n\n",
    "    distance = []\n",
    "    for i in range(len(m)):\n",
    "        distance.append(euc(m[i].reshape(-1,1), n[i].reshape(-1,1)))\n",
    "    distance = sum(distance)/len(m)\n",
    "    return distance\n",
    "\n",
    "def saveModel(weight, n):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    \n",
    "    num_classes=len(np.unique(DF_Test_Y))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "    if Gaussian_Noise == True: model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(Cluster_Size/Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "          # Define model for pruning.\n",
    "        pruning_params = {\n",
    "            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                        final_sparsity=final_sparsity,\n",
    "                                                                        begin_step=0,\n",
    "                                                                        end_step=end_step)\n",
    "          }\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "            tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.set_weights(weight)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    scores = model.evaluate(DF_Test_X, DF_Test_Y)\n",
    "\n",
    "    print(\"Saved Model Loss: \", scores[0])        #Loss\n",
    "    print(\"Saved Model Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    fpath = \"./weights/global\"+str(n)+\".h5\"\n",
    "    model.save(fpath)\n",
    "    return scores[0], scores[1]\n",
    "\n",
    "def getDataLen(trainingDict):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    n = 0\n",
    "    for w in trainingDict:\n",
    "        n += trainingDict[w]\n",
    "#     print('Total number of data points after this round: ', n)\n",
    "    return n\n",
    "\n",
    "def assignWeights(trainingDf, trainingDict):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    n = getDataLen(trainingDict)\n",
    "    trainingDf['Weightage'] = trainingDf['DataSize'].apply(lambda x: x/n)\n",
    "    return trainingDf, n\n",
    "    \n",
    "def scale(weight, scaler):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    scaledWeights = []\n",
    "    for i in range(len(weight)):\n",
    "        scaledWeights.append(scaler * weight[i])\n",
    "    return scaledWeights\n",
    "\n",
    "def getScaledWeight(d, scaler):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        model = prune_low_magnitude(model)\n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return scale(weight, scaler)\n",
    "\n",
    "def getWeight(d):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        model = prune_low_magnitude(model)\n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return weight\n",
    "\n",
    "def avgWeights(scaledWeights):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    avg = list()\n",
    "    for weight_list_tuple in zip(*scaledWeights):\n",
    "        layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0)\n",
    "        avg.append(layer_mean)\n",
    "    return avg\n",
    "\n",
    "def FedAvg(trainingDict):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize']) \n",
    "    models = list(trainingDict.keys())\n",
    "    scaledWeights = []\n",
    "    trainingDf, dataLen = assignWeights(trainingDf, trainingDict)\n",
    "    for m in models:\n",
    "        scaledWeights.append(getScaledWeight(m, trainingDf.loc[m]['Weightage']))\n",
    "    fedAvgWeight = avgWeights(scaledWeights)\n",
    "    return fedAvgWeight, dataLen\n",
    "\n",
    "def MK(trainingDict, b):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    print('MK Training Dict: ', trainingDict)\n",
    "    models = list(trainingDict.keys())\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize'])\n",
    "    l_weights = []\n",
    "    g_weight = {}\n",
    "    for m in models:\n",
    "        if 'global' in m:\n",
    "            g_weight['name'] = m\n",
    "            g_weight['weight'] = getWeight(m)\n",
    "        else:\n",
    "            l_weights.append({\n",
    "                'name': m,\n",
    "                'weight': getWeight(m)\n",
    "            })\n",
    "    \n",
    "    scores = {}\n",
    "#     if (g_weight == {}):\n",
    "#         return -1,-1\n",
    "    for m in l_weights:\n",
    "        scores[m['name']] = euclidean(m['weight'], g_weight['weight'])\n",
    "    sortedScores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1])}\n",
    "\n",
    "    b = int(len(scores)*b)\n",
    "    \n",
    "    selected = []\n",
    "    for i in range(b):\n",
    "        selected.append((sortedScores.popitem())[0])\n",
    "\n",
    "    newDict = {}\n",
    "    for i in trainingDict.keys():\n",
    "        if (((i not in selected) and ('global' not in i))):\n",
    "            newDict[i] = trainingDict[i]\n",
    "\n",
    "    print('Selections: ', newDict)\n",
    "    NewGlobal, dataLen = FedAvg(newDict)\n",
    "    return NewGlobal, dataLen\n",
    "\n",
    "def TransferLearn(name, X_train, X_test,y_train,y_test):\n",
    "    global curr_global\n",
    "    global curr_local\n",
    "    # X_train, y_train = preprocess(traindf)\n",
    "    # X_test, y_test = preprocess(testdf)\n",
    "\n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[0], X_train.shape[1], y_train.shape[0]\n",
    "\n",
    "    inner_model = Sequential(\n",
    "    [\n",
    "        Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1))\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(inner_model)\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(n_timesteps / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "              'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                       final_sparsity=final_sparsity,\n",
    "                                                                       begin_step=0,\n",
    "                                                                       end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "          tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "          tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.load_weights(\"./weights/global\"+str(curr_global)+\".h5\")\n",
    "\n",
    "    if Gradient_Clipping == True:\n",
    "        opt = keras.optimizers.Adam(clipnorm=Clip_Norm)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    for layer in inner_model.layers:#freezing layers to retain the weights \n",
    "        layer.trainable = False\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        history = model.fit(X_train, y_train, epochs=TL_Epochs, batch_size=Batch_Size, verbose=1, callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, epochs=TL_Epochs, batch_size=Batch_Size, verbose=1)\n",
    "\n",
    "    history2 = model.evaluate(X_test, y_test, batch_size=Batch_Size, verbose=1)\n",
    "\n",
    "    test_accuracy = history2[1]\n",
    "    print(f\"Subject Test Accuracy Post Transfer Learning is {test_accuracy}\")\n",
    "\n",
    "    nofedAcc = get_subject_testacc_before_TL(X_test, y_test, curr_global)\n",
    "\n",
    "    getCent = get_own_individual_acc(X_train, y_train, curr_global)\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\"+str(name)+\".h5\")\n",
    "    return n_timesteps, model\n",
    "\n",
    "def get_subject_testacc_before_TL(X_test, y_test, curr_global):\n",
    "    \n",
    "    global curr_local\n",
    "#     global curr_global\n",
    "        \n",
    "    num_classes=len(np.unique(y_test))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(Cluster_Size / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                    final_sparsity=final_sparsity,\n",
    "                                                                    begin_step=0,\n",
    "                                                                    end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    model.load_weights(\"./weights/global\"+str(curr_global)+\".h5\")\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "      \n",
    "    scores = model.evaluate(X_test,y_test)\n",
    "    print(\"Subject Testing Loss before TL: \", scores[0])        #Loss\n",
    "    print(\"Subject Testing Accuracy before TL: \", scores[1])    #Accuracy\n",
    "    \n",
    "def get_own_individual_acc(X_train, y_train, curr_global):\n",
    "    \n",
    "    global curr_local\n",
    "#     global curr_global\n",
    "\n",
    "    X_test, y_test = DF_Test_X,DF_Test_Y\n",
    "\n",
    "    num_classes=len(np.unique(y_test))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28,28,1)))\n",
    "\n",
    "    if Gaussian_Noise == True:\n",
    "        model.add(GaussianNoise(Gaussian_Noise_Std_Dev))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10,activation=\"softmax\"))#2 represent output layer neurons\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        end_step = np.ceil(16500 / Batch_Size).astype(np.int32) * NUM_Epoch\n",
    "\n",
    "        # Define model for pruning.\n",
    "        pruning_params = {\n",
    "            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                    final_sparsity=final_sparsity,\n",
    "                                                                    begin_step=0,\n",
    "                                                                    end_step=end_step)\n",
    "        }\n",
    "\n",
    "        logdir = tempfile.mkdtemp()\n",
    "\n",
    "        callbacks = [\n",
    "            tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "            tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "        ]\n",
    "\n",
    "        model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # model.load_weights(\"./weights/global\"+str(curr_global)+\".h5\")\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if Gradient_Pruning == True:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1, callbacks=callbacks)\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train, batch_size=Batch_Size, epochs=NUM_Epoch, verbose=1)\n",
    "      \n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    print(\"Invidual Loss without TL: \", scores[0])        #Loss\n",
    "    print(\"invidual Accuracy without TL: \", scores[1])    #Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    global NUM_Clients,Cluster_Size,Batch_Size,NUM_Epoch,verbose,krum_f,Gaussian_Noise,Gaussian_Noise_Std_Dev,Gradient_Clipping,Clip_Norm,Gradient_Pruning,initial_sparsity,final_sparsity\n",
    "    \n",
    "    num_iter = 2\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        print(f\"-----------------------Repeating entire process the {i}th time-------------------\")\n",
    "        global curr_local\n",
    "        global curr_global\n",
    "        \n",
    "        print('----------------------------------------')\n",
    "        print('Number of Clients: ', NUM_Clients)\n",
    "        print('Cluster Size: ', Cluster_Size)\n",
    "        print('Batch Size: ', Batch_Size)\n",
    "        print('Number of Local Epochs: ', NUM_Epoch)\n",
    "        print('F: ', krum_f)\n",
    "        print('Gaussian_Noise: ', Gaussian_Noise)\n",
    "        if Gaussian_Noise: \n",
    "            print('Noise Std Dev: ', Gaussian_Noise_Std_Dev)\n",
    "        print('Gradient_Clipping: ', Gradient_Clipping)\n",
    "        if Gradient_Clipping: \n",
    "            print('Clip Norm: ', Clip_Norm)\n",
    "        print('Gradient_Pruning: ', Gradient_Pruning)\n",
    "        if Gradient_Pruning: \n",
    "            print('Pruning Sparcity: ', final_sparsity)\n",
    "        print('----------------------------------------')\n",
    "        \n",
    "        curr_local = 0\n",
    "        curr_global = 0\n",
    "        \n",
    "        clients = {}\n",
    "        \n",
    "        Outer_Xtrain,Outer_Xtest,Outer_Ytrain,Outer_Ytest = DF_Train_X,DF_Test_X,DF_Train_Y,DF_Test_Y #happens at global level, only 70% is shared w clients, rest is kept for testing\n",
    "\n",
    "        per_client_data = (len(Outer_Xtrain)//NUM_Clients)\n",
    "        for i in range(NUM_Clients):\n",
    "            \n",
    "            clients[f'X_{i}'] = Outer_Xtrain[per_client_data*i: per_client_data*(i+1)]\n",
    "            clients[f'Y_{i}'] = Outer_Ytrain[per_client_data*i: per_client_data*(i+1)]\n",
    "            clients[f'X_train_{i}'],clients[f'X_test_{i}'],clients[f'Y_train_{i}'],clients[f'Y_test_{i}'] = split_70_30(clients[f'X_{i}'],clients[f'Y_{i}']) \n",
    "\n",
    "        local = {}\n",
    "        loss_array = []\n",
    "        acc_array = []\n",
    "        curr_datalen = 0\n",
    "        \n",
    "        for i in range(0, len(Outer_Xtrain), Cluster_Size):\n",
    "            curr_datalen += Cluster_Size\n",
    "            print(\"Total Data Used: \", curr_datalen)\n",
    "            \n",
    "            if int(curr_global) == 0:\n",
    "                curr_global += 1\n",
    "                print('Current Global: ', curr_global)\n",
    "                name = 'global' + str(curr_global)\n",
    "                X_train_fed, y_train_fed = Outer_Xtrain[i:i+Cluster_Size],Outer_Ytrain[i:i+Cluster_Size]\n",
    "                l, m = train(name, X_train_fed, y_train_fed, curr_global)\n",
    "                local[name] = l\n",
    "                \n",
    "            else:\n",
    "                print('Current Local: ', curr_local)\n",
    "                name = str('local'+str(curr_local))\n",
    "                curr_local += 1\n",
    "                X_train_fed, y_train_fed = Outer_Xtrain[i:i+Cluster_Size],Outer_Ytrain[i:i+Cluster_Size]\n",
    "                if X_train_fed.shape[0]<=Cluster_Size-1:\n",
    "                    break\n",
    "                l, m = train(name, X_train_fed, y_train_fed, curr_global)\n",
    "                local[name] = l\n",
    "                \n",
    "                if (int(curr_local)% NUM_Clients == 0):\n",
    "                    curr_global += 1\n",
    "                    print('Current Global: ', curr_global)\n",
    "                    name = 'global' + str(curr_global)\n",
    "                    m, l = MK(local, krum_f)\n",
    "                    loss, acc = saveModel(m, curr_global)\n",
    "                    loss_array.append(loss)\n",
    "                    acc_array.append(acc)\n",
    "                    local = {}\n",
    "                    local[name] = l\n",
    "\n",
    "        print(\"Global Accuracy Array: \", acc_array)\n",
    "          \n",
    "                \n",
    "        for j in range(NUM_Clients):\n",
    "            print(\"calling tflearn for client \", j)\n",
    "            TransferLearn(f\"C{j}\", clients[f'X_train_{j}'], clients[f'X_test_{j}'],clients[f'Y_train_{j}'],clients[f'Y_test_{j}'])\n",
    "\n",
    "        NUM_Clients+=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------Repeating entire process the 0th time-------------------\n",
      "----------------------------------------\n",
      "Number of Clients:  10\n",
      "Cluster Size:  1000\n",
      "Batch Size:  32\n",
      "Number of Local Epochs:  1\n",
      "F:  0.0\n",
      "Gaussian_Noise:  False\n",
      "Gradient_Clipping:  False\n",
      "Gradient_Pruning:  False\n",
      "----------------------------------------\n",
      "Total Data Used:  1000\n",
      "Current Global:  1\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 2.2062 - accuracy: 0.1905\n",
      "Total Data Used:  2000\n",
      "Current Local:  0\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.2177 - accuracy: 0.1797\n",
      "Total Data Used:  3000\n",
      "Current Local:  1\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 2.2089 - accuracy: 0.2607\n",
      "Total Data Used:  4000\n",
      "Current Local:  2\n",
      "32/32 [==============================] - 2s 28ms/step - loss: 2.2001 - accuracy: 0.2180\n",
      "Total Data Used:  5000\n",
      "Current Local:  3\n",
      "32/32 [==============================] - 2s 34ms/step - loss: 2.1690 - accuracy: 0.2477\n",
      "Total Data Used:  6000\n",
      "Current Local:  4\n",
      "32/32 [==============================] - 2s 24ms/step - loss: 2.2365 - accuracy: 0.1899\n",
      "Total Data Used:  7000\n",
      "Current Local:  5\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.1875 - accuracy: 0.2050\n",
      "Total Data Used:  8000\n",
      "Current Local:  6\n",
      "32/32 [==============================] - 2s 22ms/step - loss: 2.1862 - accuracy: 0.2233\n",
      "Total Data Used:  9000\n",
      "Current Local:  7\n",
      "32/32 [==============================] - 2s 24ms/step - loss: 2.2311 - accuracy: 0.2123\n",
      "Total Data Used:  10000\n",
      "Current Local:  8\n",
      "32/32 [==============================] - 2s 24ms/step - loss: 2.1329 - accuracy: 0.2740\n",
      "Total Data Used:  11000\n",
      "Current Local:  9\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.2099 - accuracy: 0.2212\n",
      "Current Global:  2\n",
      "MK Training Dict:  {'global1': 1000, 'local0': 1000, 'local1': 1000, 'local2': 1000, 'local3': 1000, 'local4': 1000, 'local5': 1000, 'local6': 1000, 'local7': 1000, 'local8': 1000, 'local9': 1000}\n",
      "Selections:  {'local0': 1000, 'local1': 1000, 'local2': 1000, 'local3': 1000, 'local4': 1000, 'local5': 1000, 'local6': 1000, 'local7': 1000, 'local8': 1000, 'local9': 1000}\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 2.3007 - accuracy: 0.1160\n",
      "Saved Model Loss:  2.300901174545288\n",
      "Saved Model Accuracy:  0.11349999904632568\n",
      "Total Data Used:  12000\n",
      "Current Local:  10\n",
      "32/32 [==============================] - 2s 30ms/step - loss: 2.2863 - accuracy: 0.1293\n",
      "Total Data Used:  13000\n",
      "Current Local:  11\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.2952 - accuracy: 0.1350\n",
      "Total Data Used:  14000\n",
      "Current Local:  12\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.2967 - accuracy: 0.1408\n",
      "Total Data Used:  15000\n",
      "Current Local:  13\n",
      "32/32 [==============================] - 2s 23ms/step - loss: 2.2895 - accuracy: 0.1442\n",
      "Total Data Used:  16000\n",
      "Current Local:  14\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 2.2766 - accuracy: 0.1991\n",
      "Total Data Used:  17000\n",
      "Current Local:  15\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.2721 - accuracy: 0.2023\n",
      "Total Data Used:  18000\n",
      "Current Local:  16\n",
      "32/32 [==============================] - 2s 25ms/step - loss: 2.2853 - accuracy: 0.1564\n",
      "Total Data Used:  19000\n",
      "Current Local:  17\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.2787 - accuracy: 0.1976\n",
      "Total Data Used:  20000\n",
      "Current Local:  18\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 2.2834 - accuracy: 0.1880\n",
      "Total Data Used:  21000\n",
      "Current Local:  19\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.2803 - accuracy: 0.1498\n",
      "Current Global:  3\n",
      "MK Training Dict:  {'global2': 10000, 'local10': 1000, 'local11': 1000, 'local12': 1000, 'local13': 1000, 'local14': 1000, 'local15': 1000, 'local16': 1000, 'local17': 1000, 'local18': 1000, 'local19': 1000}\n",
      "Selections:  {'local10': 1000, 'local11': 1000, 'local12': 1000, 'local13': 1000, 'local14': 1000, 'local15': 1000, 'local16': 1000, 'local17': 1000, 'local18': 1000, 'local19': 1000}\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.9666 - accuracy: 0.4091\n",
      "Saved Model Loss:  1.9618510007858276\n",
      "Saved Model Accuracy:  0.41280001401901245\n",
      "Total Data Used:  22000\n",
      "Current Local:  20\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 1.6617 - accuracy: 0.4381\n",
      "Total Data Used:  23000\n",
      "Current Local:  21\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.7437 - accuracy: 0.4115\n",
      "Total Data Used:  24000\n",
      "Current Local:  22\n",
      "32/32 [==============================] - 2s 26ms/step - loss: 1.7278 - accuracy: 0.4219\n",
      "Total Data Used:  25000\n",
      "Current Local:  23\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.7256 - accuracy: 0.4061\n",
      "Total Data Used:  26000\n",
      "Current Local:  24\n",
      "32/32 [==============================] - 2s 37ms/step - loss: 1.6484 - accuracy: 0.4675\n",
      "Total Data Used:  27000\n",
      "Current Local:  25\n",
      "32/32 [==============================] - 3s 38ms/step - loss: 1.7402 - accuracy: 0.3912\n",
      "Total Data Used:  28000\n",
      "Current Local:  26\n",
      "32/32 [==============================] - 3s 39ms/step - loss: 1.7157 - accuracy: 0.4091 0s - loss: 1.7490 - accuracy\n",
      "Total Data Used:  29000\n",
      "Current Local:  27\n",
      "32/32 [==============================] - 2s 34ms/step - loss: 1.6853 - accuracy: 0.4060\n",
      "Total Data Used:  30000\n",
      "Current Local:  28\n",
      "32/32 [==============================] - 2s 34ms/step - loss: 1.7343 - accuracy: 0.4376 0s - loss:\n",
      "Total Data Used:  31000\n",
      "Current Local:  29\n",
      "32/32 [==============================] - 2s 30ms/step - loss: 1.7855 - accuracy: 0.4069\n",
      "Current Global:  4\n",
      "MK Training Dict:  {'global3': 10000, 'local20': 1000, 'local21': 1000, 'local22': 1000, 'local23': 1000, 'local24': 1000, 'local25': 1000, 'local26': 1000, 'local27': 1000, 'local28': 1000, 'local29': 1000}\n",
      "Selections:  {'local20': 1000, 'local21': 1000, 'local22': 1000, 'local23': 1000, 'local24': 1000, 'local25': 1000, 'local26': 1000, 'local27': 1000, 'local28': 1000, 'local29': 1000}\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 1.1755 - accuracy: 0.5972\n",
      "Saved Model Loss:  1.1421970129013062\n",
      "Saved Model Accuracy:  0.6162999868392944\n",
      "Total Data Used:  32000\n",
      "Current Local:  30\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 1.3039 - accuracy: 0.5408\n",
      "Total Data Used:  33000\n",
      "Current Local:  31\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.3414 - accuracy: 0.5317\n",
      "Total Data Used:  34000\n",
      "Current Local:  32\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1.2107 - accuracy: 0.5715\n",
      "Total Data Used:  35000\n",
      "Current Local:  33\n",
      "32/32 [==============================] - 2s 33ms/step - loss: 1.3874 - accuracy: 0.5372\n",
      "Total Data Used:  36000\n",
      "Current Local:  34\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.1798 - accuracy: 0.5682\n",
      "Total Data Used:  37000\n",
      "Current Local:  35\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.2291 - accuracy: 0.5383\n",
      "Total Data Used:  38000\n",
      "Current Local:  36\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 1.3786 - accuracy: 0.5098\n",
      "Total Data Used:  39000\n",
      "Current Local:  37\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.1961 - accuracy: 0.5531\n",
      "Total Data Used:  40000\n",
      "Current Local:  38\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.2625 - accuracy: 0.5261\n",
      "Total Data Used:  41000\n",
      "Current Local:  39\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.2664 - accuracy: 0.5570\n",
      "Current Global:  5\n",
      "MK Training Dict:  {'global4': 10000, 'local30': 1000, 'local31': 1000, 'local32': 1000, 'local33': 1000, 'local34': 1000, 'local35': 1000, 'local36': 1000, 'local37': 1000, 'local38': 1000, 'local39': 1000}\n",
      "Selections:  {'local30': 1000, 'local31': 1000, 'local32': 1000, 'local33': 1000, 'local34': 1000, 'local35': 1000, 'local36': 1000, 'local37': 1000, 'local38': 1000, 'local39': 1000}\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.9469 - accuracy: 0.6749\n",
      "Saved Model Loss:  0.8938007950782776\n",
      "Saved Model Accuracy:  0.7016000151634216\n",
      "Total Data Used:  42000\n",
      "Current Local:  40\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0704 - accuracy: 0.6309\n",
      "Total Data Used:  43000\n",
      "Current Local:  41\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1.1164 - accuracy: 0.6055\n",
      "Total Data Used:  44000\n",
      "Current Local:  42\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.0286 - accuracy: 0.6219\n",
      "Total Data Used:  45000\n",
      "Current Local:  43\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.0434 - accuracy: 0.6160\n",
      "Total Data Used:  46000\n",
      "Current Local:  44\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9854 - accuracy: 0.6525\n",
      "Total Data Used:  47000\n",
      "Current Local:  45\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.0885 - accuracy: 0.5911\n",
      "Total Data Used:  48000\n",
      "Current Local:  46\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.0905 - accuracy: 0.6107\n",
      "Total Data Used:  49000\n",
      "Current Local:  47\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.0502 - accuracy: 0.6297\n",
      "Total Data Used:  50000\n",
      "Current Local:  48\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.0780 - accuracy: 0.6091\n",
      "Total Data Used:  51000\n",
      "Current Local:  49\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.9864 - accuracy: 0.6444\n",
      "Current Global:  6\n",
      "MK Training Dict:  {'global5': 10000, 'local40': 1000, 'local41': 1000, 'local42': 1000, 'local43': 1000, 'local44': 1000, 'local45': 1000, 'local46': 1000, 'local47': 1000, 'local48': 1000, 'local49': 1000}\n",
      "Selections:  {'local40': 1000, 'local41': 1000, 'local42': 1000, 'local43': 1000, 'local44': 1000, 'local45': 1000, 'local46': 1000, 'local47': 1000, 'local48': 1000, 'local49': 1000}\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 0.7662 - accuracy: 0.7525\n",
      "Saved Model Loss:  0.7191988229751587\n",
      "Saved Model Accuracy:  0.7746999859809875\n",
      "Total Data Used:  52000\n",
      "Current Local:  50\n",
      "32/32 [==============================] - 2s 22ms/step - loss: 0.8120 - accuracy: 0.7240\n",
      "Total Data Used:  53000\n",
      "Current Local:  51\n",
      "32/32 [==============================] - 2s 36ms/step - loss: 0.8958 - accuracy: 0.7255\n",
      "Total Data Used:  54000\n",
      "Current Local:  52\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.8733 - accuracy: 0.7096\n",
      "Total Data Used:  55000\n",
      "Current Local:  53\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.8607 - accuracy: 0.7205\n",
      "Total Data Used:  56000\n",
      "Current Local:  54\n",
      "32/32 [==============================] - 2s 38ms/step - loss: 0.7347 - accuracy: 0.7442\n",
      "Total Data Used:  57000\n",
      "Current Local:  55\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 0.7913 - accuracy: 0.7304\n",
      "Total Data Used:  58000\n",
      "Current Local:  56\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 0.8767 - accuracy: 0.6644\n",
      "Total Data Used:  59000\n",
      "Current Local:  57\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 0.6072 - accuracy: 0.8062\n",
      "Total Data Used:  60000\n",
      "Current Local:  58\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.7770 - accuracy: 0.7301\n",
      "Global Accuracy Array:  [0.11349999904632568, 0.41280001401901245, 0.6162999868392944, 0.7016000151634216, 0.7746999859809875]\n",
      "calling tflearn\n",
      "132/132 [==============================] - 3s 21ms/step - loss: 0.7437 - accuracy: 0.7487\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.4454 - accuracy: 0.8639\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8638888597488403\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.7257 - accuracy: 0.7659\n",
      "Subject Testing Loss before TL:  0.7432979345321655\n",
      "Subject Testing Accuracy before TL:  0.7655555605888367\n",
      "132/132 [==============================] - 3s 23ms/step - loss: 1.5376 - accuracy: 0.4705\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2470 - accuracy: 0.9296\n",
      "Invidual Loss without TL:  0.247003436088562\n",
      "invidual Accuracy without TL:  0.9296000003814697\n",
      "calling tflearn\n",
      "132/132 [==============================] - 4s 25ms/step - loss: 0.7959 - accuracy: 0.7296\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.4832 - accuracy: 0.8517\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8516666889190674\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.7338 - accuracy: 0.7700\n",
      "Subject Testing Loss before TL:  0.7329691648483276\n",
      "Subject Testing Accuracy before TL:  0.7755555510520935\n",
      "132/132 [==============================] - 4s 24ms/step - loss: 1.5721 - accuracy: 0.4764\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3111 - accuracy: 0.9040\n",
      "Invidual Loss without TL:  0.311107337474823\n",
      "invidual Accuracy without TL:  0.9039999842643738\n",
      "calling tflearn\n",
      "132/132 [==============================] - 4s 24ms/step - loss: 0.8322 - accuracy: 0.7209\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.5106 - accuracy: 0.8406\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8405555486679077\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 0.7841 - accuracy: 0.7458\n",
      "Subject Testing Loss before TL:  0.7846699953079224\n",
      "Subject Testing Accuracy before TL:  0.7411110997200012\n",
      "132/132 [==============================] - 4s 23ms/step - loss: 1.6117 - accuracy: 0.4510\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3232 - accuracy: 0.8883\n",
      "Invidual Loss without TL:  0.32324299216270447\n",
      "invidual Accuracy without TL:  0.8883000016212463\n",
      "calling tflearn\n",
      "132/132 [==============================] - 4s 24ms/step - loss: 0.7613 - accuracy: 0.7550\n",
      "57/57 [==============================] - 1s 8ms/step - loss: 0.4470 - accuracy: 0.8661\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8661110997200012\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.7233 - accuracy: 0.7871\n",
      "Subject Testing Loss before TL:  0.7342712879180908\n",
      "Subject Testing Accuracy before TL:  0.7788888812065125\n",
      "132/132 [==============================] - 4s 24ms/step - loss: 1.5152 - accuracy: 0.4834\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2575 - accuracy: 0.9214\n",
      "Invidual Loss without TL:  0.25748151540756226\n",
      "invidual Accuracy without TL:  0.9214000105857849\n",
      "calling tflearn\n",
      "132/132 [==============================] - 4s 24ms/step - loss: 0.7930 - accuracy: 0.7472\n",
      "57/57 [==============================] - 1s 15ms/step - loss: 0.5224 - accuracy: 0.8222\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8222222328186035\n",
      "57/57 [==============================] - 1s 14ms/step - loss: 0.7383 - accuracy: 0.7428\n",
      "Subject Testing Loss before TL:  0.7441539168357849\n",
      "Subject Testing Accuracy before TL:  0.754444420337677\n",
      "132/132 [==============================] - 5s 25ms/step - loss: 1.5229 - accuracy: 0.4749\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2844 - accuracy: 0.9198\n",
      "Invidual Loss without TL:  0.28441452980041504\n",
      "invidual Accuracy without TL:  0.9197999835014343\n",
      "calling tflearn\n",
      "132/132 [==============================] - 3s 22ms/step - loss: 0.7887 - accuracy: 0.7217\n",
      "57/57 [==============================] - 1s 9ms/step - loss: 0.5099 - accuracy: 0.8372\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8372222185134888\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 0.8019 - accuracy: 0.7404\n",
      "Subject Testing Loss before TL:  0.7757272720336914\n",
      "Subject Testing Accuracy before TL:  0.7527777552604675\n",
      "132/132 [==============================] - 4s 23ms/step - loss: 1.5913 - accuracy: 0.4715\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2795 - accuracy: 0.9150\n",
      "Invidual Loss without TL:  0.2794850766658783\n",
      "invidual Accuracy without TL:  0.9150000214576721\n",
      "calling tflearn\n",
      "132/132 [==============================] - 4s 25ms/step - loss: 0.7822 - accuracy: 0.7364\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.4832 - accuracy: 0.8400\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8399999737739563\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 0.7534 - accuracy: 0.7677\n",
      "Subject Testing Loss before TL:  0.7224572896957397\n",
      "Subject Testing Accuracy before TL:  0.777222216129303\n",
      "132/132 [==============================] - 4s 24ms/step - loss: 1.5627 - accuracy: 0.4808\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.2886 - accuracy: 0.9117\n",
      "Invidual Loss without TL:  0.288638710975647\n",
      "invidual Accuracy without TL:  0.9117000102996826\n",
      "calling tflearn\n",
      "132/132 [==============================] - 3s 22ms/step - loss: 0.7798 - accuracy: 0.7379\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.5190 - accuracy: 0.8417\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8416666388511658\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.7632 - accuracy: 0.7512\n",
      "Subject Testing Loss before TL:  0.8035654425621033\n",
      "Subject Testing Accuracy before TL:  0.7400000095367432\n",
      "132/132 [==============================] - 4s 25ms/step - loss: 1.5136 - accuracy: 0.4723\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2543 - accuracy: 0.9253\n",
      "Invidual Loss without TL:  0.2543022334575653\n",
      "invidual Accuracy without TL:  0.9253000020980835\n",
      "calling tflearn\n",
      "132/132 [==============================] - 4s 24ms/step - loss: 0.7766 - accuracy: 0.7432\n",
      "57/57 [==============================] - 1s 7ms/step - loss: 0.4595 - accuracy: 0.8694\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8694444298744202\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 0.7098 - accuracy: 0.7711\n",
      "Subject Testing Loss before TL:  0.7312442064285278\n",
      "Subject Testing Accuracy before TL:  0.7688888907432556\n",
      "132/132 [==============================] - 4s 23ms/step - loss: 1.5389 - accuracy: 0.4777\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2727 - accuracy: 0.9208\n",
      "Invidual Loss without TL:  0.2726663649082184\n",
      "invidual Accuracy without TL:  0.920799970626831\n",
      "calling tflearn\n",
      "132/132 [==============================] - 3s 22ms/step - loss: 0.7238 - accuracy: 0.7624\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 0.3898 - accuracy: 0.8839\n",
      "Subject Test Accuracy Post Transfer Learning is 0.8838889002799988\n",
      "57/57 [==============================] - 1s 6ms/step - loss: 0.7139 - accuracy: 0.7978\n",
      "Subject Testing Loss before TL:  0.6773272752761841\n",
      "Subject Testing Accuracy before TL:  0.7994444370269775\n",
      "132/132 [==============================] - 3s 22ms/step - loss: 1.4991 - accuracy: 0.4939\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2486 - accuracy: 0.9263\n",
      "Invidual Loss without TL:  0.2486373782157898\n",
      "invidual Accuracy without TL:  0.9262999892234802\n",
      "-----------------------Repeating entire process the 1th time-------------------\n",
      "----------------------------------------\n",
      "Number of Clients:  15\n",
      "Cluster Size:  1000\n",
      "Batch Size:  32\n",
      "Number of Local Epochs:  1\n",
      "F:  0.0\n",
      "Gaussian_Noise:  False\n",
      "Gradient_Clipping:  False\n",
      "Gradient_Pruning:  False\n",
      "----------------------------------------\n",
      "Total Data Used:  1000\n",
      "Current Global:  1\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 2.1988 - accuracy: 0.2662\n",
      "Total Data Used:  2000\n",
      "Current Local:  0\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.2262 - accuracy: 0.1805\n",
      "Total Data Used:  3000\n",
      "Current Local:  1\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.1829 - accuracy: 0.2623\n",
      "Total Data Used:  4000\n",
      "Current Local:  2\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 2.1867 - accuracy: 0.2239\n",
      "Total Data Used:  5000\n",
      "Current Local:  3\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 2.1892 - accuracy: 0.2369 0s - loss: 2.2696 - accu\n",
      "Total Data Used:  6000\n",
      "Current Local:  4\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.1913 - accuracy: 0.1956\n",
      "Total Data Used:  7000\n",
      "Current Local:  5\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 2.1627 - accuracy: 0.2281\n",
      "Total Data Used:  8000\n",
      "Current Local:  6\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 2.2174 - accuracy: 0.1966\n",
      "Total Data Used:  9000\n",
      "Current Local:  7\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.1807 - accuracy: 0.2489\n",
      "Total Data Used:  10000\n",
      "Current Local:  8\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 2.1417 - accuracy: 0.2273\n",
      "Total Data Used:  11000\n",
      "Current Local:  9\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 2.1582 - accuracy: 0.2483\n",
      "Total Data Used:  12000\n",
      "Current Local:  10\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.1795 - accuracy: 0.2209\n",
      "Total Data Used:  13000\n",
      "Current Local:  11\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 2.1844 - accuracy: 0.2170\n",
      "Total Data Used:  14000\n",
      "Current Local:  12\n",
      "32/32 [==============================] - 2s 26ms/step - loss: 2.2319 - accuracy: 0.1885\n",
      "Total Data Used:  15000\n",
      "Current Local:  13\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 2.1871 - accuracy: 0.2312\n",
      "Total Data Used:  16000\n",
      "Current Local:  14\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 2.1413 - accuracy: 0.2560\n",
      "Current Global:  2\n",
      "MK Training Dict:  {'global1': 1000, 'local0': 1000, 'local1': 1000, 'local2': 1000, 'local3': 1000, 'local4': 1000, 'local5': 1000, 'local6': 1000, 'local7': 1000, 'local8': 1000, 'local9': 1000, 'local10': 1000, 'local11': 1000, 'local12': 1000, 'local13': 1000, 'local14': 1000}\n",
      "Selections:  {'local0': 1000, 'local1': 1000, 'local2': 1000, 'local3': 1000, 'local4': 1000, 'local5': 1000, 'local6': 1000, 'local7': 1000, 'local8': 1000, 'local9': 1000, 'local10': 1000, 'local11': 1000, 'local12': 1000, 'local13': 1000, 'local14': 1000}\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 2.3005 - accuracy: 0.1326\n",
      "Saved Model Loss:  2.3006434440612793\n",
      "Saved Model Accuracy:  0.1347000002861023\n",
      "Total Data Used:  17000\n",
      "Current Local:  15\n",
      "32/32 [==============================] - 2s 31ms/step - loss: 2.2434 - accuracy: 0.1265\n",
      "Total Data Used:  18000\n",
      "Current Local:  16\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 2.2718 - accuracy: 0.1621\n",
      "Total Data Used:  19000\n",
      "Current Local:  17\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 2.2599 - accuracy: 0.1580\n",
      "Total Data Used:  20000\n",
      "Current Local:  18\n",
      "32/32 [==============================] - 2s 29ms/step - loss: 2.2555 - accuracy: 0.1334 0s - loss: 2.2757 - accuracy: \n",
      "Total Data Used:  21000\n",
      "Current Local:  19\n",
      "32/32 [==============================] - 2s 30ms/step - loss: 2.2730 - accuracy: 0.1660\n",
      "Total Data Used:  22000\n",
      "Current Local:  20\n",
      "32/32 [==============================] - 2s 21ms/step - loss: 2.2450 - accuracy: 0.1565\n",
      "Total Data Used:  23000\n",
      "Current Local:  21\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 2.2748 - accuracy: 0.1884\n",
      "Total Data Used:  24000\n",
      "Current Local:  22\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 2.2599 - accuracy: 0.1442\n",
      "Total Data Used:  25000\n",
      "Current Local:  23\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 2.2432 - accuracy: 0.2215\n",
      "Total Data Used:  26000\n",
      "Current Local:  24\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.2566 - accuracy: 0.1412\n",
      "Total Data Used:  27000\n",
      "Current Local:  25\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 2.2524 - accuracy: 0.1384\n",
      "Total Data Used:  28000\n",
      "Current Local:  26\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.2536 - accuracy: 0.1503\n",
      "Total Data Used:  29000\n",
      "Current Local:  27\n",
      "32/32 [==============================] - 2s 26ms/step - loss: 2.2556 - accuracy: 0.1326\n",
      "Total Data Used:  30000\n",
      "Current Local:  28\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 2.2769 - accuracy: 0.1275\n",
      "Total Data Used:  31000\n",
      "Current Local:  29\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 2.2540 - accuracy: 0.2019\n",
      "Current Global:  3\n",
      "MK Training Dict:  {'global2': 15000, 'local15': 1000, 'local16': 1000, 'local17': 1000, 'local18': 1000, 'local19': 1000, 'local20': 1000, 'local21': 1000, 'local22': 1000, 'local23': 1000, 'local24': 1000, 'local25': 1000, 'local26': 1000, 'local27': 1000, 'local28': 1000, 'local29': 1000}\n",
      "Selections:  {'local15': 1000, 'local16': 1000, 'local17': 1000, 'local18': 1000, 'local19': 1000, 'local20': 1000, 'local21': 1000, 'local22': 1000, 'local23': 1000, 'local24': 1000, 'local25': 1000, 'local26': 1000, 'local27': 1000, 'local28': 1000, 'local29': 1000}\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 1.7017 - accuracy: 0.4045\n",
      "Saved Model Loss:  1.690961480140686\n",
      "Saved Model Accuracy:  0.3977999985218048\n",
      "Total Data Used:  32000\n",
      "Current Local:  30\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1.6276 - accuracy: 0.4085\n",
      "Total Data Used:  33000\n",
      "Current Local:  31\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.6352 - accuracy: 0.3993\n",
      "Total Data Used:  34000\n",
      "Current Local:  32\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.5312 - accuracy: 0.4605\n",
      "Total Data Used:  35000\n",
      "Current Local:  33\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.6374 - accuracy: 0.4113\n",
      "Total Data Used:  36000\n",
      "Current Local:  34\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.6050 - accuracy: 0.4178\n",
      "Total Data Used:  37000\n",
      "Current Local:  35\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.5827 - accuracy: 0.4110\n",
      "Total Data Used:  38000\n",
      "Current Local:  36\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.6543 - accuracy: 0.3876\n",
      "Total Data Used:  39000\n",
      "Current Local:  37\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 1.5547 - accuracy: 0.4515\n",
      "Total Data Used:  40000\n",
      "Current Local:  38\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.5682 - accuracy: 0.4545\n",
      "Total Data Used:  41000\n",
      "Current Local:  39\n",
      "32/32 [==============================] - 2s 27ms/step - loss: 1.5053 - accuracy: 0.4638\n",
      "Total Data Used:  42000\n",
      "Current Local:  40\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 1.6546 - accuracy: 0.4179\n",
      "Total Data Used:  43000\n",
      "Current Local:  41\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.6204 - accuracy: 0.4428\n",
      "Total Data Used:  44000\n",
      "Current Local:  42\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.5820 - accuracy: 0.4515\n",
      "Total Data Used:  45000\n",
      "Current Local:  43\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.6643 - accuracy: 0.3715\n",
      "Total Data Used:  46000\n",
      "Current Local:  44\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.5896 - accuracy: 0.4187\n",
      "Current Global:  4\n",
      "MK Training Dict:  {'global3': 15000, 'local30': 1000, 'local31': 1000, 'local32': 1000, 'local33': 1000, 'local34': 1000, 'local35': 1000, 'local36': 1000, 'local37': 1000, 'local38': 1000, 'local39': 1000, 'local40': 1000, 'local41': 1000, 'local42': 1000, 'local43': 1000, 'local44': 1000}\n",
      "Selections:  {'local30': 1000, 'local31': 1000, 'local32': 1000, 'local33': 1000, 'local34': 1000, 'local35': 1000, 'local36': 1000, 'local37': 1000, 'local38': 1000, 'local39': 1000, 'local40': 1000, 'local41': 1000, 'local42': 1000, 'local43': 1000, 'local44': 1000}\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 1.1965 - accuracy: 0.5930\n",
      "Saved Model Loss:  1.1464041471481323\n",
      "Saved Model Accuracy:  0.6218000054359436\n",
      "Total Data Used:  47000\n",
      "Current Local:  45\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.3188 - accuracy: 0.5178\n",
      "Total Data Used:  48000\n",
      "Current Local:  46\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1.2930 - accuracy: 0.5472\n",
      "Total Data Used:  49000\n",
      "Current Local:  47\n",
      "32/32 [==============================] - 2s 25ms/step - loss: 1.2408 - accuracy: 0.5199\n",
      "Total Data Used:  50000\n",
      "Current Local:  48\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.4335 - accuracy: 0.4929\n",
      "Total Data Used:  51000\n",
      "Current Local:  49\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.2795 - accuracy: 0.5376\n",
      "Total Data Used:  52000\n",
      "Current Local:  50\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1.2828 - accuracy: 0.5457\n",
      "Total Data Used:  53000\n",
      "Current Local:  51\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.3194 - accuracy: 0.5494\n",
      "Total Data Used:  54000\n",
      "Current Local:  52\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.2636 - accuracy: 0.5217\n",
      "Total Data Used:  55000\n",
      "Current Local:  53\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.2323 - accuracy: 0.5504\n",
      "Total Data Used:  56000\n",
      "Current Local:  54\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1.1754 - accuracy: 0.5658\n",
      "Total Data Used:  57000\n",
      "Current Local:  55\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.2722 - accuracy: 0.5751\n",
      "Total Data Used:  58000\n",
      "Current Local:  56\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1.2911 - accuracy: 0.5154\n",
      "Total Data Used:  59000\n",
      "Current Local:  57\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.1740 - accuracy: 0.5877\n",
      "Total Data Used:  60000\n",
      "Current Local:  58\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.2527 - accuracy: 0.5561\n",
      "Global Accuracy Array:  [0.1347000002861023, 0.3977999985218048, 0.6218000054359436]\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 1.1819 - accuracy: 0.5857\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7243 - accuracy: 0.7708\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7708333134651184\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.1362 - accuracy: 0.6188\n",
      "Subject Testing Loss before TL:  1.1611944437026978\n",
      "Subject Testing Accuracy before TL:  0.6141666769981384\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 1.8218 - accuracy: 0.3794\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3438 - accuracy: 0.9057\n",
      "Invidual Loss without TL:  0.3438034653663635\n",
      "invidual Accuracy without TL:  0.9057000279426575\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.1945 - accuracy: 0.5727\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.8444 - accuracy: 0.7317\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7316666841506958\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 1.1699 - accuracy: 0.6136\n",
      "Subject Testing Loss before TL:  1.1841624975204468\n",
      "Subject Testing Accuracy before TL:  0.6108333468437195\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.7666 - accuracy: 0.4141\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3800 - accuracy: 0.8977\n",
      "Invidual Loss without TL:  0.3799842894077301\n",
      "invidual Accuracy without TL:  0.8977000117301941\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.1725 - accuracy: 0.5929\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.7453 - accuracy: 0.7867\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7866666913032532\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 1.1246 - accuracy: 0.6191\n",
      "Subject Testing Loss before TL:  1.119568943977356\n",
      "Subject Testing Accuracy before TL:  0.6399999856948853\n",
      "88/88 [==============================] - 3s 21ms/step - loss: 1.8184 - accuracy: 0.3734\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.4675 - accuracy: 0.8530\n",
      "Invidual Loss without TL:  0.46748271584510803\n",
      "invidual Accuracy without TL:  0.8529999852180481\n",
      "calling tflearn\n",
      "88/88 [==============================] - 3s 21ms/step - loss: 1.3082 - accuracy: 0.5407 0s - loss: 1.3631 - \n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7707 - accuracy: 0.7442\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7441666722297668\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 1.1287 - accuracy: 0.6479\n",
      "Subject Testing Loss before TL:  1.1572258472442627\n",
      "Subject Testing Accuracy before TL:  0.6274999976158142\n",
      "88/88 [==============================] - 3s 24ms/step - loss: 1.9020 - accuracy: 0.3326\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3880 - accuracy: 0.8850\n",
      "Invidual Loss without TL:  0.38795697689056396\n",
      "invidual Accuracy without TL:  0.8849999904632568\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 1.2145 - accuracy: 0.5629\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7368 - accuracy: 0.7400\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7400000095367432\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 1.1798 - accuracy: 0.6035\n",
      "Subject Testing Loss before TL:  1.1709651947021484\n",
      "Subject Testing Accuracy before TL:  0.6150000095367432\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.7696 - accuracy: 0.4018\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3790 - accuracy: 0.8960\n",
      "Invidual Loss without TL:  0.378951758146286\n",
      "invidual Accuracy without TL:  0.8960000276565552\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.1776 - accuracy: 0.6056\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 0.7617 - accuracy: 0.7358\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7358333468437195\n",
      "38/38 [==============================] - 1s 6ms/step - loss: 1.1955 - accuracy: 0.6100\n",
      "Subject Testing Loss before TL:  1.1923493146896362\n",
      "Subject Testing Accuracy before TL:  0.6200000047683716\n",
      "88/88 [==============================] - 2s 22ms/step - loss: 1.7693 - accuracy: 0.3909\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3312 - accuracy: 0.9029\n",
      "Invidual Loss without TL:  0.331165075302124\n",
      "invidual Accuracy without TL:  0.902899980545044\n",
      "calling tflearn\n",
      "88/88 [==============================] - 3s 22ms/step - loss: 1.2082 - accuracy: 0.5758\n",
      "38/38 [==============================] - 1s 9ms/step - loss: 0.7575 - accuracy: 0.7275\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7275000214576721\n",
      "38/38 [==============================] - 1s 8ms/step - loss: 1.1531 - accuracy: 0.6223\n",
      "Subject Testing Loss before TL:  1.1753923892974854\n",
      "Subject Testing Accuracy before TL:  0.6066666841506958\n",
      "88/88 [==============================] - 3s 22ms/step - loss: 1.8025 - accuracy: 0.3809\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.4031 - accuracy: 0.8870\n",
      "Invidual Loss without TL:  0.40313324332237244\n",
      "invidual Accuracy without TL:  0.8870000243186951\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.2135 - accuracy: 0.5749\n",
      "38/38 [==============================] - 1s 9ms/step - loss: 0.7561 - accuracy: 0.7575\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7574999928474426\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 1.1745 - accuracy: 0.6039\n",
      "Subject Testing Loss before TL:  1.1730653047561646\n",
      "Subject Testing Accuracy before TL:  0.6108333468437195\n",
      "88/88 [==============================] - 2s 19ms/step - loss: 1.8089 - accuracy: 0.3871\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.4155 - accuracy: 0.8732\n",
      "Invidual Loss without TL:  0.41546201705932617\n",
      "invidual Accuracy without TL:  0.873199999332428\n",
      "calling tflearn\n",
      "88/88 [==============================] - 3s 23ms/step - loss: 1.1861 - accuracy: 0.5760\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.8132 - accuracy: 0.7358\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7358333468437195\n",
      "38/38 [==============================] - 1s 8ms/step - loss: 1.1948 - accuracy: 0.6280\n",
      "Subject Testing Loss before TL:  1.187853217124939\n",
      "Subject Testing Accuracy before TL:  0.6083333492279053\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.7890 - accuracy: 0.4170\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3773 - accuracy: 0.8871\n",
      "Invidual Loss without TL:  0.3772870600223541\n",
      "invidual Accuracy without TL:  0.8870999813079834\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.2046 - accuracy: 0.5760\n",
      "38/38 [==============================] - 1s 9ms/step - loss: 0.8040 - accuracy: 0.7267\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7266666889190674\n",
      "38/38 [==============================] - 1s 10ms/step - loss: 1.1617 - accuracy: 0.6113\n",
      "Subject Testing Loss before TL:  1.1485557556152344\n",
      "Subject Testing Accuracy before TL:  0.60916668176651\n",
      "88/88 [==============================] - 3s 25ms/step - loss: 1.7183 - accuracy: 0.4130\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3663 - accuracy: 0.8857\n",
      "Invidual Loss without TL:  0.36630892753601074\n",
      "invidual Accuracy without TL:  0.885699987411499\n",
      "calling tflearn\n",
      "88/88 [==============================] - 3s 23ms/step - loss: 1.2200 - accuracy: 0.5630\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.9078 - accuracy: 0.6883\n",
      "Subject Test Accuracy Post Transfer Learning is 0.6883333325386047\n",
      "38/38 [==============================] - 1s 6ms/step - loss: 1.2379 - accuracy: 0.6154\n",
      "Subject Testing Loss before TL:  1.2237412929534912\n",
      "Subject Testing Accuracy before TL:  0.6150000095367432\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.7672 - accuracy: 0.3925\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3911 - accuracy: 0.8784\n",
      "Invidual Loss without TL:  0.39111873507499695\n",
      "invidual Accuracy without TL:  0.8784000277519226\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 1.1843 - accuracy: 0.5673\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.7797 - accuracy: 0.7175\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7174999713897705\n",
      "38/38 [==============================] - 0s 7ms/step - loss: 1.2411 - accuracy: 0.5895\n",
      "Subject Testing Loss before TL:  1.2002758979797363\n",
      "Subject Testing Accuracy before TL:  0.6075000166893005\n",
      "88/88 [==============================] - 2s 17ms/step - loss: 1.8249 - accuracy: 0.3901\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.4147 - accuracy: 0.8728\n",
      "Invidual Loss without TL:  0.4147034287452698\n",
      "invidual Accuracy without TL:  0.8727999925613403\n",
      "calling tflearn\n",
      "88/88 [==============================] - 3s 25ms/step - loss: 1.1833 - accuracy: 0.5742\n",
      "38/38 [==============================] - 1s 9ms/step - loss: 0.8263 - accuracy: 0.7317\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7316666841506958\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 1.2249 - accuracy: 0.6086\n",
      "Subject Testing Loss before TL:  1.1997686624526978\n",
      "Subject Testing Accuracy before TL:  0.6075000166893005\n",
      "88/88 [==============================] - 3s 24ms/step - loss: 1.7948 - accuracy: 0.4177 1s - los - ETA: 0s - loss: 1.8987 - ac\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.4287 - accuracy: 0.8548\n",
      "Invidual Loss without TL:  0.4286811053752899\n",
      "invidual Accuracy without TL:  0.8547999858856201\n",
      "calling tflearn\n",
      "88/88 [==============================] - 3s 23ms/step - loss: 1.1744 - accuracy: 0.5892\n",
      "38/38 [==============================] - 1s 8ms/step - loss: 0.7621 - accuracy: 0.7492\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7491666674613953\n",
      "38/38 [==============================] - 1s 7ms/step - loss: 1.1442 - accuracy: 0.6270\n",
      "Subject Testing Loss before TL:  1.1390042304992676\n",
      "Subject Testing Accuracy before TL:  0.637499988079071\n",
      "88/88 [==============================] - 3s 22ms/step - loss: 1.7785 - accuracy: 0.3820\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.3893 - accuracy: 0.8908\n",
      "Invidual Loss without TL:  0.3892546594142914\n",
      "invidual Accuracy without TL:  0.8907999992370605\n",
      "calling tflearn\n",
      "88/88 [==============================] - 2s 20ms/step - loss: 1.0768 - accuracy: 0.6276\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.6736 - accuracy: 0.7675\n",
      "Subject Test Accuracy Post Transfer Learning is 0.7674999833106995\n",
      "38/38 [==============================] - 1s 9ms/step - loss: 1.1433 - accuracy: 0.6413\n",
      "Subject Testing Loss before TL:  1.1216318607330322\n",
      "Subject Testing Accuracy before TL:  0.6474999785423279\n",
      "88/88 [==============================] - 3s 23ms/step - loss: 1.6914 - accuracy: 0.4348\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3898 - accuracy: 0.8944\n",
      "Invidual Loss without TL:  0.38978949189186096\n",
      "invidual Accuracy without TL:  0.8944000005722046\n"
     ]
    }
   ],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
